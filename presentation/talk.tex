\documentclass[10pt,usepdftitle=false,aspectratio=169]{beamer}


\input{preamble_talk}
\renewcommand{\L}{\mathcal{L}}
\setbeameroption{show notes on second screen = right}
\renewcommand{\figurewidth}{\textwidth / 2}

\usetikzlibrary{external}
\tikzexternalize[mode=list and make]
% use  make -j 8 -f talk.makefile to compile with 8 parallel threads (this is what it takes to max out the machine, depsite it having 4 cores)
%\tikzset{external/force remake=true}
\tikzsetexternalprefix{fig/external/}

\begin{document}
	
	\tikzexternaldisable
	\begin{frame}
	\title{{\bf Bachelor Thesis}\newline Investigating Probabilistic Preconditioning on Deep Learning using DeepOBS
		\vspace*{-.7cm}}
	\author{Ludwig Bald\vspace{-1cm}} \date{T\"ubingen\\\today}
	
	\vspace{-1.5cm}
	\maketitle 
	\vspace{-1.0cm}

	
	\thispagestyle{empty}
	\setcounter{framenumber}{0}
	
	\tikzifexternalizing{}{%
		\begin{tikzpicture}[remember picture,overlay]
		\node[anchor=south,yshift=-5mm] at (current page.south) 
		{\includegraphics[width=0.9995\paperwidth]{assets/logo_TU_169_0.pdf}};
		\end{tikzpicture}%
	}%
	
\end{frame}
%\tikzexternalenable
\note[itemize]{
	\item Introduce myself, I study Cognitive Science
	\item I chose this topic because I wanted to get to know deep learning from the inside. I had never done Deep Learning before and wanted to learn it hands-on.
	\item In this talk I will talk about the science, but also about the process I used.
	\item If you have questions during the talk, ask them right away!
}


\setlength{\figwidth}{.9\textwidth}
\setlength{\figheight}{.6\textheight}

\section{Background}

\begin{frame}
\frametitle{Machine Learning}
\framesubtitle{Background}
	\begin{block}{Definition \footnote{Tom Mitchell, "Machine Learning", 1997}}
		"A computer program is said to learn from \emph{experience E} with respect to some class of \emph{tasks T} and \emph{performance measure P} if its performance at tasks in T, as measured by P, improves with experience E"
	\end{block}
\begin{itemize}
	\item Special case in Deep Learning: Finding the \emph{optimal point} in \emph{parameter space}
	\item The \emph{Loss function $\L (w)$} tells us how bad a specific parametrization $w$ is. For example, the model's predictions for all possible inputs are compared to the desired outputs.
	% TODO image of 2-dimensional problem
\end{itemize}
\end{frame}
\note[itemize]{
	\item Most of you will have seen this definition before.
	\item As an example, have a look at this 2-parametrical problem
}

\begin{frame}
\frametitle{Gradient Descent}
\framesubtitle{Optimization}
	Gradient Descent uses knowledge of the \emph{gradient} at a point in parameter space to take an update step:
		$$ w_{i+1} = w_i - \alpha \cdot \nabla \L(w_i) $$
	where $\alpha$ is the learning rate.
	
\end{frame}

\begin{frame}
\frametitle{Stochastic Gradient Descent}
\framesubtitle{Optimization}
\begin{itemize}
	\item In real life, we have Big Data. The true $\nabla \L(w)$ is expensive to compute.
	\item To speed things up, we compute the noisy estimate $\hat{\L}(w_i)$ on a minibatch of for example 128 data points.
\end{itemize}
The update rule still looks the same:
$$w_{i+1} = w_i - \alpha \cdot \nabla \hat{\L}(w_i) $$
where $\alpha$ is the learning rate.

\end{frame}


\begin{frame}
\frametitle{Preconditioning}
\framesubtitle{The condition number of the Hessian}
	\begin{itemize}
		\item The performance of (S)GD depends heavily on the shape of the loss landscape
		\item The \emph{condition number} is defined as $$\kappa = \frac{\lambda_n}{\lambda_1} > 1$$
			 where $\lambda_n, \lambda_1$ are the largest/smallest eigenvalues of the Hessian $\nabla \nabla \L(w)$
		\item For larger $\kappa$, (S)GD can converge slower.
		\item The condition number can be changed by carefully rescaling the gradient before taking the optimization step
		%TODO example image!
	\end{itemize}
\end{frame}

\begin{frame}{Probabilistic Preconditioning}{by Filip \& Philipp, 2019}
In the stochastic (minibatched) setting and while only having access to \emph{Hessian-vector products}, it isn't obvious how to construct the preconditioner. This is the method I'm testing:
\begin{enumerate}
	\item Empirically construct a prior for the multivariate Gaussian distribution and set the learning rate for SGD
	\item Gather observations and update the posterior estimate for the Hessian, using Bayes
	\item Create a rank-2 approximation of the Preconditioner
	\item apply the preconditioner at every step and do SGD
\end{enumerate}
\end{frame}
\note{If I'm grossly misrepresenting the algorithm, please correct me now! For an exact description check out the paper}

\begin{frame}{Deep learning}{Neural nets}
	For the purposes of this talk, a neural net is a model
	\begin{itemize}
		\item with many ( $>$ hundreds of thousands) parameters, weights $w$
		\item with an available noisy gradient $\nabla \hat\L(w_0)$, which was obtained by backpropagation
	\end{itemize}
\end{frame}

\section{Approach}
\begin{frame}{Evaluating an Optimizer}{Empirically}
	This is a hard problem in itself! How do you chose:
	\begin{itemize}
		\item Data Set
		\item Batch Size
		\item Model architecture
		\item Hyperparameter tuning method
		\item Measure of success
	\end{itemize}
	Comparing results between papers is very hard.
\end{frame}
\note{Now that we have roughly defined the algorithm, how do we test it?}

\begin{frame}{DeepOBS}{by Frank \& Aaron}
	\begin{itemize}
		\item A library for Tensorflow and Pytorch
		\item In order to test an optimizer, you have to specify only
			\begin{itemize}
			\item The optimizer class
			\item The hyperparameters of my optimizer
			\item One of the provided testproblems
			\end{itemize}
		\item DeepOBS then returns a json file
		\item And automatically generates figures
	\end{itemize}
\end{frame}

\begin{frame}[fragile]{Implementation Details}
	\framesubtitle{The class Preconditioner}
\begin{verbatim}
Preconditioner(params, est_rank=2, num_observations=5, prior_iterations=10,
               weight_decay=0, lr=None,
               optim_class=torch.optim.SGD, **optim_hyperparams)
start_estimate()
step()
get_log()
\end{verbatim}	
\end{frame}

\begin{frame}{How to use the TCML Cluster}{Cloud Computing}
\begin{enumerate}
	\item Request an account by sending an email
	\item If you have any special code requirements, build a Singularity container (kind of like a virtual machine). Alternatively use a provided one.
	\item Create \& Submit a Slurm Batch job file
	\item Get an e-mail when your jobs start of finish
	\item Download the output files to your local machine. You can mount the cluster as a virtual drive.
\end{enumerate}
\end{frame}
	

\section{Experiments + Results + Discussion}
\begin{frame}{Experiments}{Overview}
\begin{itemize}
	\item Effectiveness of Preconditioning
	\item Computational Complexity
	\item Stability
	\item Learning Rate sensitivity
\end{itemize}
\end{frame}

\begin{frame}{Effectiveness of Preconditioning}{\tikz\draw[orange,fill=orange] (0,0) circle (.7ex); AdaptiveSGD \hspace{1cm} \tikz\draw[white,fill=blue] (0,0) circle (.7ex); PreconditionedSGD}
\vspace{3mm}
\input{images/exp_preconditioning.pgf}
\end{frame}

\begin{frame}{Effectiveness of Preconditioning}{\tikz\draw[orange,fill=orange] (0,0) circle (.7ex); AdaptiveSGD \hspace{1cm} \tikz\draw[white,fill=blue] (0,0) circle (.7ex); PreconditionedSGD}
\input{images/exp_preconditioning2.pgf}
\end{frame}

\begin{frame}{Discussion: Effectiveness}{Why might it be worse than plain SGD?}
	\begin{itemize}
		\item It's very noisy: Noise from the Hessian is amplified through the whole epoch.
		\item The constructed learning rate is not optimal
		\item PreconditionedSGD is worse than AdaptiveSGD, so the Preconditioning makes things worse
		\item The preconditioner has only rank 2, while there might be thousands large eigenvalues (usually ~10\%)
		\item The other optimizers are exhaustively tuned
		\item The other optimizers use more data for actual parameter updates: 1920/50.000 images per epoch are only used for the Hessian
	\end{itemize}
\end{frame}

\begin{frame}{Performance penalty}{Computational overhead}
\centering
\input{../thesis/images/exp_perf_prec.pgf}
\end{frame}

%\begin{frame}{Stability depending on initialization}
%%TODO Do I want to keep this? & include figure
%\end{frame}

\begin{frame}{Learning Rate Sensitivity}{vs SGD}
\centering
\input{images/exp_lr_sens.pgf}
\end{frame}

\begin{frame}{Conclusion/Final Remarks}{}

\end{frame}


\blackslidetext{end of presentation}


\end{document}