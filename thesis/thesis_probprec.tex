%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% LaTeX-Rahmen fuer das Erstellen von Masterarbeiten
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% allgemeine Einstellungen
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[twoside,12pt,a4paper]{report}
%\usepackage{reportpage}
\usepackage{epsf}
\usepackage{graphics, graphicx}
\usepackage{latexsym}
\usepackage[margin=10pt,font=small,labelfont=bf]{caption}
\usepackage[utf8]{inputenc}
\usepackage[toc,page]{appendix}
\usepackage{todonotes}
\usepackage{markdown}
\usepackage[ngerman, english]{babel}
\usepackage[hidelinks]{hyperref}

\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\usepgfplotslibrary{groupplots}
\usepgfplotslibrary{dateplot}


\textwidth 14cm
\textheight 22cm
\topmargin 0.0cm
\evensidemargin 1cm
\oddsidemargin 1cm
%\footskip 2cm
\parskip0.5explus0.1exminus0.1ex

\newcommand{\figurewidth}{\linewidth / 2}
\newcommand{\done}[2][]{\todo[color=green!40, #1]{\textbf{Done:} #2}}

% Kann von Student auch nach pers\"onlichem Geschmack ver\"andert werden.
\pagestyle{headings}

\sloppy


\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% hier steht die neue Titelseite 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{titlepage}
	\begin{center}
		{\LARGE Eberhard Karls Universit\"at T\"ubingen}\\
		{\large Mathematisch-Naturwissenschaftliche Fakult\"at \\
			Wilhelm-Schickard-Institut f\"ur Informatik\\[4cm]}
		{\huge Bachelor Thesis Cognitive Science\\[2cm]}
		{\Large\bf  Investigating Probabilistic Preconditioning on Artificial Neural Networks \\[1.5cm]}
		{\large Ludwig Bald}\\[0.5cm]
		\today \\[4cm]
		\parbox{7cm}{\begin{center}{{\small\bf Gutachter}\\
				[0.5cm]\large Prof. Dr. Philipp Hennig}\\
				(Methoden des Maschinellen Lernens)\\
				{\footnotesize Wilhelm-Schickard-Institut f\"ur Informatik\\
				Universit\"at T\"ubingen}\end{center}}\hfill\parbox{7cm}
				{\begin{center}
				{\small\bf Betreuer}\\[0.5cm]
				{\large Filip De Roos}\\
				(Methoden des Maschinellen Lernens)\\
				{\footnotesize Wilhelm-Schickard-Institut f\"ur Informatik\\
					Universit\"at T\"ubingen}\end{center}
		}
	\end{center}
\end{titlepage}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Titelr"uckseite: Bibliographische Angaben
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\thispagestyle{empty}
\vspace*{\fill}
\begin{minipage}{11.2cm}
	\textbf{Bald, Ludwig:}\\
	\emph{Investigating Probabilistic Preconditioning on Artificial Neural Networks}\\ Bachelor Thesis Cognitive Science\\
	Eberhard Karls Universit\"at T\"ubingen\\
	Thesis period: von-bis
\end{minipage}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagenumbering{roman}
\setcounter{page}{1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Seite I: Zusammenfassug, Danksagung
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section*{Abstract}
\todo[inline]{Note to the reader: This thesis is unfinished and I put zero confidence in its correctness, quality of citations or completeness.}
\todo{theabstract, citing!}
In machine learning, stochastic gradient descent is a widely used optimizsation algorithm, used to update the parameters of a model after a minibatch of data has been observed, in order to improve the model's predictions. It has been shown to converge much faster when the condition number (i.e. the ratio between the largest and the smallest eigenvalue) of ... is closer to 1. A preconditioner reduces the condition value.
Tho goal of this thesis was to reimplement the algorithm as an easy-to-use-class in python and take part in the development of DeepOBS, by being able to give feedback as a naive user of the benchmarking suite's features.
In this thesis I present my implementation of the probabilistic preconditioning algorithm proposed in \cite{roos2019active}. I use DeepOBS \cite{deepobs} as a benchmarking toolbox, examining the effect of this kind of preconditioning on various optimizers and test problems. 
The results...

\newpage
\section*{Zusammenfassung}
\todo{Abstract auf Deutsch}

\newpage
\section*{Acknowledgments}
If you have someone to Acknowledge ;)
\todo{Aaron, Filip, Frank, Felix}
\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Table of Contents
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\renewcommand{\baselinestretch}{1.3}
\small\normalsize

\tableofcontents

\renewcommand{\baselinestretch}{1}
\small\normalsize

\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Der Haupttext, ab hier mit arabischer Numerierung
%%% Mit \input{dateiname} werden die Datei `dateiname' eingebunden
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagenumbering{arabic}
\setcounter{page}{1}


\chapter{Introduction}
What is this all about?


\chapter{Fundamentals and Related Work}
\section{Probability Basics}
One important concept in probability theory is the Gaussian distribution.
The real-valued \textit{normal} or \textit{Gaussian} distribution is defined as:
$$N(x | \mu, \sigma^2) = \frac{1}{(2\pi \sigma^2)^{1/2}} \exp\left(- \frac{1}{2\sigma^2} (x-\mu)^2 \right)$$
It has the parameters \textit{mean} $\mu$ and \textit{variance} $\sigma ^2$.

Extending this to a \textit{multivariate} Gaussian distribution with $n$ dimensions, the definition needs to be adapted:
$$N(\vec{x} | \vec{\mu}, \Sigma) = \frac{1}{(2\pi)^{n/2}} \frac{1}{det(\Sigma)^{1/2}} \exp\left(-\frac{1}{2} (\vec{x} - \vec{\mu})^T \Sigma ^{-1} (\vec{x} - \vec{\mu})\right)$$
where $\vec{x}$ or simply $x$ is the $n$-dimensional vector of random variables. The parameters are similar to the one-dimensional case. The $n$-dimensional vector $\vec{\mu}$ or simply $\mu$ denotes the mean. The symmetric $n \times n$ matrix $\Sigma$ is the \textit{covariance matrix}, where
$$ \Sigma_{ij} =  cov(x_i, x_j) = \Sigma_{ji}$$
For the diagonal entries it follows that:
$$\Sigma_{ii} = cov(x_i,x_i) = var(x_i)$$
If the variables $x_i$ are independent,$\Sigma$ is a diagonal matrix.

Matrix-valued Normal distributions can be written as vector-valued normal distributions by flattening the matrix to a vector.
\todo{cite the book}

\begin{markdown}
- Point estimates
- Bayes' Rule
\end{markdown}

\section{Machine Learning}
\todo[inline]{Machine Learning intro}
\begin{markdown}
- recent changes: big data, specialized hardware
- Machine Learning is used everywhere and really important
	- key factor in automatization.
- The process of machine learning
	- Gathering data
		- Splitting the data (Train, Test, validation)
	- Designing a model
		- Hyperparameter tuning
	- **Training the model**
	- Application
	
\subsection{Loss functions}
How do we recognize the optimal solution, or even tell which of two solutions is better?
Depending on the task that the model is meant to perform, we can define different so-called Loss Functions (also called "risk"). These are typically some sort of distance measure between the model's output and the desired, output.
However, there are other values we care about, such as the model's ability to generalize, i.e. how well it performs on data it has never seen before. If a model performs well on the trainig data set, but fails to generalize, this is called overfitting and means the model is not very useful on new data. The model memorizes the data, but doesn't get the underlying structure. To combat overfitting, people add a regularization term to the Loss function, which penalizes large parameters, which are usually an indicator that the model is overfitting.
\todo[inline]{formula}
\todo[inline]{split data}

\end{markdown}

\section{Optimization}
In order to find the optimal values for the parameters of a model, different methods have been proposed. For low-dimensional optimization problems with a small number of parameters, it is often possible to find the optimal parametrization analytically. For high-dimensional optimization problems, the analytical solution often is computationally intractable. As a result, numerical optimization algorithms have emerged.
These algorithms use the available limited information in order to iteratively approximate the optimal parametrization. The number of iterations needed until the algorithm converges varies based on the implicit prior assumptions about the model.

\subsection{First-Order and Second-Order Methods}
For many models, it is possible to obtain information about the derivatives of the loss function \todo{symbol} at a given point in parameter-space. Optimization algorithms can be grouped by the order of the highest-order derivative they use.

There are zeroth-order methods that only require the value of the loss function itself. 

First-order methods use the first derivative, or the gradient. Adaptive Modifications of SGD like Adam and Momentum keep track of past gradients in order to take better steps. This implicitly assumes that past gradients contain information about future gradients, which is often the case. For example, if there is little change in the last gradients, we can assume that we are far away from the minimum, where gradients should converge to zero. Therefore, we are able to take larger steps.

Second-Order-Methods are methods that explicitly use the Hessian $B$ of the Loss function, or the second derivative. This means they can use the explicit representation of curvature to directly compute where the gradient will hit zero.

-RMSprop
-Adam
- Conjugate gradient?
s.o.:
-Newton: Needs access to the Hessian.
-Quasi-Newton.
We can keep track of an estimate of the Hessian along the way.


Some of these optimization processes have "hyperparameters" (different from the models "parameters" that we want to optimize).

\subsection{Gradient Descent}
If the gradient of the Loss function at a certain point in parameter space is known, we can use this information to update the parameters in a way that takes us closer to the solution. A popular family of algorithms is derived from SGD. SGD means: Just take a step in the direction of steepest gradient. Scale the step size by the steepness of the gradient. If the gradient is very steep, take a larger step. If it is small, take a smaller step, like a drunk student tumbling down a hill and ending up on a local minimum.
SGD has only a single hyperparameter, the "learning rate" $\alpha$, which is multiplied with the step. Its optimal choice depends on the data, the model and is generally not obvious.
\done{formula: Update Rule}
$$w_{i+1} = w_i - \alpha \cdot \nabla L(w_i) $$

\subsection{Stochastic Gradient Descent}
Traditional Gradient Descent uses the whole data set to compute the true gradient, which is computationally instractable for large datasets, the usual case in machine learning. Instead, we use a variation called SGD. We compute only an estimate for the true gradient by "minibatching", using only a few, for example 64 data points in the forward pass. This greatly improves convergence speed, as the required computations are much easier to perform. However, especially for smaller batch sizes, this adds noise to the system, meaning that our parameter update step points only roughly in the direction of the steepest actual gradient. It has also been shown to improve generalization of the model.
Many variants of SGD have been proposed, adding things like a momentum term or otherwise adapting the learning rate dynamically.
\done{formula: Noisy update rule}

$$w_{i+1} = w_i - \alpha \cdot \nabla \hat{L}(w_i) $$


\section{Preconditioning}
Convergence speed depends on the \textit{condition number} $\kappa = \frac{\lambda_n}{\lambda_1}$ of the Hessian.


The performance of SGD does not only depend on the choice of the learning rate, but also on the structure of the Loss landscape. The gradient of the loss function is a jacobian matrix of partial derivatives. Imagine an optimization proble with two parameters and a loss landscape that looks like an ellipse. If the random starting point is chosen to be towards the direction of the longer symmetry axis, the gradient will be rather flat and SGD will take a long time to converge. With the same logic, a circular, bowl-shaped loss landscape is optimal.
If we start at the steep wall, SGD will converge quite quickly. If you know the explicit form of the Hessian matrix, you can see this problem by comparing its eigenvalues. A circular, bowl-shaped loss function will have only eigenvalues that are the same size. In the elliptical case, which has previously been reported to be the standard case in machine learning, at least one eigenvalue is much larger than the others.
The mathematical measure for this phenomenon is called the "condition number" and is defined as the ratio between the largest and the smallest eigenvalues. SGD converges much faster when the condition number is closer to 1. The largest eigenvalue is sometimes referred to as the "spectral radius".
\todo[inline]{derivation of why the condition number is important, Limit on convergence}
$$ \frac{\kappa - 1}{\kappa + 1}$$

Preconditioning is a way to reduce the condition number. A preconditioner is a matrix that rescales the other matrix in such a way that the ratio of eigenvalues approaches 1.
If the eigenvalues are known, this is quite easy. However, in deep learning, they are not known. In this thesis I present my implementation of an algorithm which aims to efficiently estimate the eigenvalues and construct a Preconditioner, while taking into account the noise caused by minibatching.

Condition number vs Spectral Radius

The PROBLEM:

Up until now, there was no easy way to make use of preconditioning in a noisy setting, such as minibatched deep learning. I present an implementation of Filips Algorithm in an easy-to-use python class and demonstrate its strengths and weaknesses.
- The convergence of optimizers depends on the condition number of the Hessian done{formula}, which describes the whole loss landscape.
\todo{Image}
\todo{Formula of how an optimal preconditioner works}
\missingfigure{This figure illustrates the condition number. It shows what SGD does on an illconditioned two-dimensional problem. And what preconditioning can do to change this problem.}

\section{Deep learning}
\subsection{Artificial Neural Networks}
A popular machine learning paradigma is living through a resurgence: Artificial Neural Networks. The fundamental building block is the single neuron, whcih somewhat resembles a biological neuron. Its activation depends on the sum of the activation of its inputs. A neural network model is a sequence of neuron layers, connected with each other. There is an input layer, which is a direct mapping of the training data point. The input layer' activation are fed forward into the "hidden layers", which in turn feed their activations to the neurons of the output layer. Information about observed data is stored in the model's parameters, the weights and biases of each layer.
As a mathematical object, a neuron is an activation function which depends on the sum of the weighted activations of the inputs. Often there is a bias, which is a static value added to the input activation.
\todo{nonlinear (affine) activation functions}
\todo[inline]{architectures per task. ANN: CNN (visual/spatial), RNN (temporal)}



\subsection{Regularization vs. weight decay}
We minimize a different parameter than what we actually care about. (Loss on the training set vs accuracy on the test set). This can lead to problems, so there are some solutions.
If the model's parameters are too large, that often means it is overfitting. in order to discourage this, a regularization term is sometimes added to the loss function. Usually this is the L2 norm of the parameter vector, but sometimes the L1 norm is also used.
- But there are differences (see \cite{DBLP:conf/iclr/ChaudhariCSLBBC17})
\todo{Bayesian?}
\todo{Formula}
\todo{Maybe illustrative plot}

\subsection{Automatic Differentiation}
\todo{Forward pass}
\todo{Derivation}
For the most interesting class of optimizers, it is required to know the gradient at the current point in parameter space. In neural networks, this is achieved by Automatic Differentiation.
Loss functions in neural networks have an interesting structure, in that they are a composition of all the layerwise activation functions. This structure leads to the algorithm of backpropagation:
During training, the model (using its current value for its parameters) is fed with data from the data set. The loss function of this minibatch is computed. While this happens, the computations that take place are tracked and built into a graph structure. This is necessary, because the loss function in practice isn't defined in a closed-form way directly on the parameters, but as a composition of layerwise activation functions. This means that for every parameter tracked by the graph, we can infer its influence on the loss. The parameters need to be leaves of the tree, which means they don't themselves depend on something else. (provide example graph image here).
The gradient in the direction of an individual leaf (partial derivative) is then computed by applying the chain rule, starting from the output of the loss function. This is called the backward pass.





\section{Benchmarking}
using deepobs \cite{deepobs}
\begin{markdown}
There are no standard established benchmarking protocols for new optimizers. It isn't even clear what measures to consider, or how they are to be measured. As a result, nobody knows which optimizers are actually good. And some bad optimizers will seem good.
DeepOBS is a solution to this problem, standardizing a protocol, providing benchmarks and standard test problems.

- Description and examples of previous optimizer plots.
- Short overview of how deepobs handles stuff:
    - Test problems:
      	- DeepOBS includes the most used standard datasets and a variety of neural network models to train. This ensures that everyone is evaluated on the same problem.
    - Tuning:
    	- Many optimizers have hyperparameters that greatly affect the optimizer's performance
 	    - These need to be tuned by running many settings seperately, for example in a grid search. The actual deepobs protocol isn't ready yet.
      - DeepOBS generates commands for the grid search.
    - Running:
      	- DeepOBS provides a standard way to run your optimizer, taking care of logging parameters and evaluating success measures.
    - Analyzing:
      	- DeepOBS provides the analyzer class, which is able to automatically generate matplotlib plots showing the results of your runs.
\end{markdown}

\section{Related Work}
\todo{in which other ways has this problem been adressed? (What even is the problem?)}


\chapter{Approach (Implementation)}
\section{Description of the algorithm}
The exact inner workings of the algorithm are described in more detail in \cite{roos2019active}.
Here I will give an overview of the algorithm's structure and steps:
part 1: - Gather observations of the curvature in-place:
part 2: - Estimating the Hessian and construct the preconditioner
part 3: - Every step, re-scale the gradients by applying the preconditioner

\subsection{Modifications of the algorithm}
The implementation and theoretical work lead to the following proposed changes to the abstract algorithm.
\subsubsection{Parameter groups}
Mainly due to technical reasons (see \todo{reference chapter}), support was added for parameter groups, but this also yields an interesting theoretical change. The algorithm already treated every parameter layer as an independent task for inversion (???), but estimated a global step size, which was the same for every parameter. With this modification, the algorithm is able to use larger step sizes for parameters that allow for it. In theory, this would allow for faster learning in the direction of those parameters. However, the time to reach total convergence relies on the slowest parameter, not on the fastest. The benefit of this approach in practice remanins to be tested.
\subsubsection{Automatic Assessment of the Hessian's quality}
In the original algorithm, the Hessian is arbitrarily re-estimated every epoch. Hessian re-estimation is needed, as demonstrated in the original paper, alpha changes over time. The goal of this modification was to expose a measure of how useful/correct the estimated Hessian is, after observing a bit of data. A new estimation proces would be started if the current estimate was worse than a certain threshold. Multiple approaches were tried. A key point of trouble was the variance-less estimate of the Hessian. The original paper mentions that the Hessian is taken as the mean of a multivariate Gaussian distribution, but it fails to take this distribution's variance into account. Instead, it just assumes this Hessian is the optimal choice.
- Comparing the predicted gradient to the actual observed gradient.

- Maybe: Automatic restart of the estimation process, once the old Hessian is determined to be out of date.

\subsubsection{Focusing on the adaptive learning rate}
Deep learning problems are very high-dimensional and usually feature many (10\%) large eigenvalues. For performance reasons, the proposed algorithm however estimates only a low-rank preconditioner. When applied, the preconditioner therefore might not reduce the problem's condition number, or might reduce it only by a little bit. It appears likely that the added computational need of constructing and repeatedly applying the preconditioner does not come with a performance benefit. Therefore I propose abandoning the preconditioning part of the algorithm. In experiment x \todo{ref} this assumption is tested.

\section{Documentation for the class Preconditioner}
\subsection{Overview}
\begin{markdown}
The class `Preconditioner` provides an easy way to use the probabilistic preconditioning algorithm proposed by \cite{roos2019active}.
This is how to use it:

1. Get the source file from the repo and include it in your project
2. Initialize the preconditioner like any other optimizer. There are reasonable default values for the hyperparameters.
3. Depending on the version you're using, manually call `start_estimate()_` at the beginning of each epoch.

In the next section there is more detailed documentation for the class, its attributes and functions.
\end{markdown}

\subsection{Methods}
\begin{markdown}
* Public functions
	* Constructor
	* `start_estimate()`
	* `step()`
	* `get_log()`
	* (`maybe_start_estimate()`)
* Private functions
	* `_initialize_lists()`
	* `_init_the_optimizer()`
	* `_gather_curvature_information()`
	* `_estimate_prior()`
	* `_setup_estimated_hessian()`
	* `_apply_estimated_inverse()`
	* `_hessian_vector_product()`
	* `_update_estimated_hessian()`
	* `_create_low_rank()`
	* `_apply_preconditioner()`
	
\end{markdown}
\subsection{Implementation Details}
\begin{markdown}
In the following, I will highlight and explain some key software design decisions I took while implementing and refactoring the algorithm provided by Filip.
The main goals for the implementation were to make the algorithm as easy to use as possible for a standard usecase, while maintaining the flexibility I needed to research specific variations. The changes to the abstract algorithm are discussed in detail in \todo{ref} The simple, default usecase was a user trying to use the preconditioner as proposed in the original paper, to optimize a neural network model. In this case, the necessary changes to the user's existing code should be minimal, with hyperparameters set to reasonable default values. For development, it is important to understand the algorithm's structure and the code and be able to easily modify the algorithm without disturbing other parts.

The code in its final form is provided as a self-contained class. According to Python best practices \todo{cite}, all the internal functions that a user should not call are marked as hidden by having names beginning with an `_`. This is an implementation of the design pattern "Low Coupling". All functions have been given descriptive names. For example, in order to start the estimation process, the function `start_estimate()` needs to be called.

The class `Preconditioner` inherits from `torch.optim.Optimizer`. This means it follows all the conventions for how optimizers are expected to behave in pytorch. This makes it intuitive to use for the pytorch user. Features that were added to support this include the `state` dict, which can be used to save and load the state of the optimizer in order to pause or continue training. Another supported feature are parameter groups. This allows to treat different parameters seperately, for example different layers of a neural network. Specifically, each Parameter group will be optimized with a seperate learning rate.

The class `Preconditioner` has a field containing an inner optimizer to be used for the actual parameter updates. In the original paper, only SGD was studied as an option, but this modification allows to use preconditioning and the adaptive learning rate estimation together with other optimizers. The Preconditioner is mostly active when estimating the Hessian. Once the estimate is complete, the class turns into a "decorator" for the provided inner optimizer class. Before the inner optimizer does its optimization step, the previously constructed preconditioner is applied to the parameters' gradient.

Logging data during training should not be a responsibility of the optimizer. In order to expose the data of interest, the class exhibits a method `get_log()`, which returns some values of interest. The user then takes care of logging and writing the data to a file outside the optimizer.

In order to change behavior of the class during development and research, the best way to make different versions is to make use of python's built-in subclassing. For example, I needed a version that skips applying the preconditioner every step, but behaves exactly the same in all other ways. This was solved by using a subclass of `Preconditioner`, which overwrites the function `_apply_preconditioner()` with an empty function.

Some bugs were also fixed. Previously, the algorithm skipped some minibatches.
\end{markdown}


\section{Test Problems}
For most experiments, the standard test problems included in DeepOBS were used, namely
The test problems used were deep learning models.
- mnist/fmnist
- cifar10
- quadratic\_deep

For some experiments, I used a seperate implementation of a cifar10 test problem.

\section{Technical details}
The experiments were run on the TCML cluster at the University of Tübingen.
A Singularity container was set up on Ubuntu 16.4 LTS with python 3.5, pytorch (version) and DeepOBS (see Appendix for Singulariy recipe).
Computation was distributed over multiple GPU compute nodes using the workload manager Slurm.
During development, I used git for distributed version control.

Other optimizers like Adam and SGD are taken from the current, but unpublished DeepOBS baselines.



\chapter{Experiment}
\section{Experiment x: Preconditioning}
- Multiple Problems: fmnist\_2c2d, cifar10\_3c3d, maybe run more
- Plot Adaptive SGD vs Preconditioned SGD vs SGD vs Adam, maybe try other baselines
\todo[inline]{Describe findings and discuss}

In order to investigate the performance benefit of the Preconditioning, the Preconditioner class was adapted to have two different versions, behaving almost exactly the same. They both compute the Hessian, they both estimate a learning rate. However only in one version the Preconditioner is applied, while in the other one it isn't. This was achieved by subclassing and overwriting the \_apply\_precenditioner() method with an empty method. Hyperparameters: Num\_observations 10, prior\_iterations 5, est\_rank 2, optim\_class torch.SGD
Results see figure \ref{fig:exp_preconditioning}. nice one right?

\begin{figure}
	%\input{images/exp_preconditioning.pgf}
	\missingfigure[figheight=23cm]{Excluded for performance reasons. Many optimizers on fmnist and cifar10}
	\caption{Both variants of the algorithm perform significantly worse than established alternatives. AdaptiveSGD is better than PreconditionedSGD. On the fmnist model, the baseline optimizers quickly reach a minimum, but then continue to overfit. AdaptiveSGD and PreconditionedSGD arrive at the mimimum much slower, but don't diverge.}
	\label{fig:exp_preconditioning}
\end{figure}

\section{Experiment x1: Performance penalty of Preconditioning}
- For a few Problems
- Use the runtime-estimation script from deepOBS


\begin{figure}
	\centering \hspace{-1,5cm}
	\input{images/exp_perf_prec.pgf}
	\caption{Wallclock time per epoch in relation to SGD. The original algorithm is the slowest, followed by the version that does everything but apply the preconditioner. Finally, the version that only computes the Prior and uses the adaptive step size is as fast as the baseline SGD.}
	\label{fig:exp_perf_prec}
	
\end{figure}


\section{Experiment y: Initialization}
- Run without initialization, without initial lr for multiple seeds
- Run with initialization, without initial lr for multiple seeds
\todo[inline]{maybe run DeepOBS mnist\_vae with different seeds, without lr}
NOT using DeepOBS built-in Testproblem!
Filip reports needing to set a manual initial learning rate for the first epoch, because the algorithm would set too big of a value when estimating the curvature on an untrained network. During testing with DeepOBS, I could not replicate this result. I believe this is due to the initialization used in DeepOBS, which was not present in Filip's code.
Results see Figure \ref{fig:exp_init}. Here?

\begin{figure}
	\centering \hspace{-1,5cm}
	\input{images/exp_init.pgf}
	\caption{PreconditionedSGD on a cifar10 net. For both batch size 64 and 128, the preconditioner converges. The initialization method has an effect, but the optimizer handles them fine (on this testproblem.)}
	\label{fig:exp_init}
	
\end{figure}


\section{Experiment z: Learning Rate Sensitivity,  Initial Learning Rate vs no initial lr}
- Preferably multiple problems: quadratic\_deep, fmnist\_2c2d, cifar10\_3c3d, maybe more
- Hyperparameter Sensitivity of PreconditionedSGD. Still have to run 10 repeats
- Plot loss over time
- Also plot NoLearningRate

However, I investigated the effect the initial learning rate has on the algorithm.
Using DeepOBS, I performed a grid search with 10 evaluations on a logarithmic grid between $10^-5$ and $10^2$ on the Dataset XXX.
see fig \ref{fig:exp_lr_sens}.
As is common in deep learning, with the learning rate over a certain threshold the model diverges in the first epoch. However, below this threshold, the initial learning rate has at most a small effect on the network's performance after 100 epochs.

\begin{figure}
	\centering \hspace{-1,5cm}
	\input{images/exp_lr_sens.pgf}
	\caption{The originally proposed algorithm has an optional hyperparameter "learning rate", which is used as SGD's learning rate in the first epoch. Below a certain model-specific threshold, it performs just as well as the constructed learning rate. Over that threshold, it diverges in the first epoch.}
	\label{fig:exp_lr_sens}
\end{figure}

\section{Experiment aa: Automatic Re-Estimation of the Hessian and optimal learning rate}
- preferably multiple problems
- Run different Automatic re-estimation models: every-epoch, every-1000
- 
\todo[inline]{come up with different approaches.}
\todo[inline]{run PreconditionedSGD: every-1000, every 1/alpha, gradnormthing, ... on multiple problems}
\todo[inline]{here be dragons} 

\section{Experiment ab: Do Parameter groups help?}
probably not i guess

%\section{Results}


\section{Discussion}
\begin{markdown}
- The algorithm greatly depends on the initialization used for the model. For some models, incorrect initialization means we can't find anything. (DeepOBS mnist\_vae, Seed 45 failed to provide anything)
- Preconditioning for the largest two eigenvectors was no better than using only the adaptive step size.
- Interestingly, a low loss doesn't always correspond to a high acccuracy.


\end{markdown}

I present a usable class for preconditioning in development frameworks 
\section{Further research/development}
\begin{markdown}
- Make the process more robust using tests and catching exceptions and so on.
- Make a more robust auto-restart feature.
- Formulate a probabilistic version of the auto-restart feature.
\end{markdown}



\chapter{Conclusion}

\appendix %% Start the appendices.
\chapter{An appendix}
Here you can insert the appendices of your thesis.

\addcontentsline{toc}{chapter}{References}
\bibliographystyle{apalike}
\bibliography{bibliography}

\begin{otherlanguage}{ngerman}
	\chapter*{Selbstst\"andigkeitserkl\"arung}
	
	Hiermit versichere ich, dass ich die vorliegende Bachelorarbeit selbst\"andig und
	nur mit den angegebenen Hilfsmitteln angefertigt habe und dass alle Stellen,
	die dem Wortlaut oder dem Sinne nach anderen Werken entnommen sind,
	durch Angaben von Quellen als Entlehnung kenntlich gemacht worden sind.
	Diese Bachelorarbeit wurde in gleicher oder \"ahnlicher Form in keinem anderen
	Studiengang als Pr\"ufungsleistung vorgelegt.
	
	\vspace*{8ex}
	\hrule
	\vspace*{2ex}
	\noindent
	Tübingen, \today \hfill Ludwig Bald
\end{otherlanguage}

\end{document}